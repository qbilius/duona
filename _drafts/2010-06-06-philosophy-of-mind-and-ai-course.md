---
title: Philosophy of Mind and AI course
date: 2010-06-06 16:31:59
categories: duona
---

**Ancient Greek** Plato: soul is immortal Phusis – natural, self-organizing Techne – artificial, can only simulate, never create **Dualism** Francis Bacon Rene Descartes: “I think therefore I am.” The existence of the mind cannot be doubted. On the other hand, the existence of body could be. Thus dualistic viewpoint. Natural is same as artificial, simple machines. But humans have something extra (mind). **Materialism** de la Mettrie: human is a mechanical machine **XX century** 1940s: first computers 194X: Hebb’s rule 1940-50s: behaviorism: the mind doesn’t matter, only observable states (inputs and output) matter 1950: Turing’s *Can machines think?* (birth of Philosophy of AI) 1956: McCarthie’s conference on AI (birth of AI as a science) 1950s: Rosenblatt’s Perceptron 1958: Type physicalism: the mind is the brain and there is nothing over and above it 1960s: Minsky’s criticism over perceptrons and distributed knowledge; still trying to stick to the traditional symbolic approach 1976: Functionalism (Hilary Putnam): hardware doesn’t matter, processes can be implemented anywhere (multiple realizability), it is the software that matters 1978: Newell and Simon: Physical Symbol System Hypothesis: Symbolic manipulations are neccessary and sufficient for intelligence to arise. 1970s: Supervenience: no mental difference without a physical one. 1980s: John Searle: syntax is neither constituent of nor sufficient for semantics, so programs cannot develop intentionality and understanding (Chinese Room). Also a separation between Strong AI (PSS hypothesis) and weak AI (that does not require for consiousness to arise). Criticisms: Churchlands: Semantics can still happen, who knows (elecromagnetism example); maybe you need parallel implementation (Searle: Chinese gym). 1980s: Roger Penrose considers Strong AI and Weak AI impossible and claims that the mind can only arise non-computationally based on a modified Godel’s argument that humans can see when a process will halt when a computer cannot. E.g.: “Find two even number a sum of which is odd.” Formally, consider all computational procedures C\_1, C\_2, …, operating on a number n. Suppose there is a procedure A(q,n) that halts when C\_q(n) does not (A does not have to produce an answer always but when it does, it must be correct). Choose q = n. Then A(n,n) operates on a single number and thus must be one of Cs, since we numbered them all. Say it is C\_k(n). Finally, choose n = k. Then A(k,k) halts when C\_k(k) does not. But also C\_k(k) halts when C\_k(k) does not. So there is a problem that a computer runs into. But we can see that C\_k(k) must be false! Criticisms. Me: number of programs is innumerable? 1990s: Dennett’s *Consciousness explained* claims that qualia do not exist (ontologically) because their properties, when looked at together, are inconsistent. Consciousness supervienes the brain in the following way: it is a collection of memes, and their implementation in the Parallel Distributed Processing System (PDP) makes it arise, while biologically it was not designed to happen. 1993: Chalmers on how a thermostat can be conscious. Marks of mental: Direct / Immediate knowledge: No medium necessary to transfer this information Privacy property / First person privilege: The pain is only yours, while a tree is not Infalsiability: “I have a toothache” – “No, you don’t” Intensionallity (aboutness): “I believe Leuven is in Belgium” Referential **My viewpoint:** You cannot exactly measure the state of the brain due to lack of precision in measurements (Heisenberg’s uncertainty principle; also Penrose). Because of that, you cannot predict the evolution of the brain states since the brain is also chaotic (highly sensitive to initial conditions). This inability to measure results in subjective states that are in principle first person experiences. There is no possible way to transfer this knowledge about a state to another person because the brain are inherently different, and there is no way to transfer this knowledge to a computer of equivalent architecture due to errors in measurements both while replicating the system and while measuring the internal states that are about to be transfered. Also, since this measurement of a state takes time (you cannot measure the entire system at once), this is an additional problem in measuring. So qualia are now redefined as a certain state of a system but that state is very private. However, the state is inherently physical and is likely defined not in absolute terms but in relation to other states. Then: Mary’s room. The difference is that knowing about \~= having experienced that state Inverted spectra. Difference in incoming wavelenght. Zombie worlds. We are zombies Chinese room. Semantics is complex syntax. Causality. Cannot be simulated due to lack in precision, Wolpert’s theorem.
